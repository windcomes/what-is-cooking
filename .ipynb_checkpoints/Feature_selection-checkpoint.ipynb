{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:14.123247Z",
     "start_time": "2019-04-01T19:55:14.110896Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:18.129624Z",
     "start_time": "2019-04-01T19:55:15.771256Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "with open('train.json', encoding='utf-8') as f:\n",
    "    d = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "data_all = pd.DataFrame(d)\n",
    "# From the file Quentin offered, we can see that if we reorder the ingredient names by its first charactor, it is \n",
    "# noted that there are some special numbers(like '1','10','15') or unit names('oz') which are meaningless.\n",
    "# Hence, the first thing is to remove these from the datasets.\n",
    "\n",
    "numbers = re.compile(r\"\\d.*\\d\\s?\")   # the Regular Expression of numbers(like '1','10','15')\n",
    "num_percent = re.compile(r\"\\d.*%\\s\") # the Regular Expression of percentage numbers like 1%\n",
    "oz = re.compile(r\"\\soz\\.\")           # the Regular Expression of 'oz.'\n",
    "seven_up = re.compile(r\"^7\\sUp\")     # the Regular Expression of '7 Up'\n",
    "\n",
    "def datasets_cleaning(list_input):\n",
    "    for ingre_no in range(len(list_input)):\n",
    "        list_input[ingre_no] = re.sub(numbers, \"\",list_input[ingre_no])\n",
    "        list_input[ingre_no] = re.sub(num_percent,\"\",list_input[ingre_no])\n",
    "        list_input[ingre_no] = re.sub(oz,\"\",list_input[ingre_no])\n",
    "        list_input[ingre_no] = re.sub(seven_up,\"7up\",list_input[ingre_no])\n",
    "    return list_input\n",
    "\n",
    "data_all['ingredients'] = data_all['ingredients'].apply(datasets_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:19.250291Z",
     "start_time": "2019-04-01T19:55:19.165794Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we seperate the dataset into train, valid, test\n",
    "y_all = data_all['cuisine'].tolist()\n",
    "X_all = data_all['ingredients'].tolist()\n",
    "\n",
    "Xtrain, Xtestval, ytrain, ytestval = train_test_split(X_all,y_all, test_size = 0.2, random_state = 42)\n",
    "Xtest, Xval, ytest, yval = train_test_split(Xtestval, ytestval, test_size = 0.5, random_state = 42)\n",
    "\n",
    "data_train = pd.DataFrame(columns=['cuisine','ingredients'])\n",
    "data_train['cuisine'] = ytrain\n",
    "data_train['ingredients'] = Xtrain # Creat a DataFrame based on train data(size: 31819*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:20.158223Z",
     "start_time": "2019-04-01T19:55:20.143464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mexican': 5102, 'indian': 2401, 'filipino': 619, 'moroccan': 655, 'korean': 664, 'irish': 516, 'southern_us': 3472, 'french': 2096, 'vietnamese': 681, 'japanese': 1139, 'italian': 6271, 'thai': 1224, 'spanish': 807, 'chinese': 2163, 'british': 647, 'brazilian': 383, 'greek': 926, 'russian': 400, 'cajun_creole': 1218, 'jamaican': 435}\n"
     ]
    }
   ],
   "source": [
    "data_train\n",
    "dict1 = {}\n",
    "for key in ytrain:\n",
    "    dict1[key] = dict1.get(key, 0) + 1\n",
    "print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:21.245302Z",
     "start_time": "2019-04-01T19:55:21.020106Z"
    }
   },
   "outputs": [],
   "source": [
    "# then we want to split the name of each ingredients into several single words.\n",
    "def tokenize_list(list_input):\n",
    "    new_list =[val.split(\" \") for val in list_input]\n",
    "    list_get = [item for sublist in new_list for item in sublist]\n",
    "    return list_get\n",
    "# example_origin = data_train['ingredients'][0]\n",
    "# example_new = tokenize_list(example_origin)\n",
    "# print(example_origin)\n",
    "# print(example_new)\n",
    "data_train['ingre_splited'] = data_train['ingredients'].apply(tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:22.204907Z",
     "start_time": "2019-04-01T19:55:22.190290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingre_splited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[shredded cheddar cheese, chicken meat, choppe...</td>\n",
       "      <td>[shredded, cheddar, cheese, chicken, meat, cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian</td>\n",
       "      <td>[fresh cilantro, purple onion, ground coriande...</td>\n",
       "      <td>[fresh, cilantro, purple, onion, ground, coria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>[sugar, garlic, onions, vinegar, green chilies...</td>\n",
       "      <td>[sugar, garlic, onions, vinegar, green, chilie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moroccan</td>\n",
       "      <td>[raw pistachios, purple onion, couscous, dried...</td>\n",
       "      <td>[raw, pistachios, purple, onion, couscous, dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[tomatoes, pepper, salsa, sliced green onions,...</td>\n",
       "      <td>[tomatoes, pepper, salsa, sliced, green, onion...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cuisine                                        ingredients  \\\n",
       "0   mexican  [shredded cheddar cheese, chicken meat, choppe...   \n",
       "1    indian  [fresh cilantro, purple onion, ground coriande...   \n",
       "2  filipino  [sugar, garlic, onions, vinegar, green chilies...   \n",
       "3  moroccan  [raw pistachios, purple onion, couscous, dried...   \n",
       "4   mexican  [tomatoes, pepper, salsa, sliced green onions,...   \n",
       "\n",
       "                                       ingre_splited  \n",
       "0  [shredded, cheddar, cheese, chicken, meat, cho...  \n",
       "1  [fresh, cilantro, purple, onion, ground, coria...  \n",
       "2  [sugar, garlic, onions, vinegar, green, chilie...  \n",
       "3  [raw, pistachios, purple, onion, couscous, dri...  \n",
       "4  [tomatoes, pepper, salsa, sliced, green, onion...  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:23.179464Z",
     "start_time": "2019-04-01T19:55:23.138136Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train['ingre_string'] = data_train['ingre_splited'].str.join(' ') # combines the splited words into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:24.100287Z",
     "start_time": "2019-04-01T19:55:24.088467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingre_splited</th>\n",
       "      <th>ingre_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[shredded cheddar cheese, chicken meat, choppe...</td>\n",
       "      <td>[shredded, cheddar, cheese, chicken, meat, cho...</td>\n",
       "      <td>shredded cheddar cheese chicken meat chopped o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian</td>\n",
       "      <td>[fresh cilantro, purple onion, ground coriande...</td>\n",
       "      <td>[fresh, cilantro, purple, onion, ground, coria...</td>\n",
       "      <td>fresh cilantro purple onion ground coriander g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>[sugar, garlic, onions, vinegar, green chilies...</td>\n",
       "      <td>[sugar, garlic, onions, vinegar, green, chilie...</td>\n",
       "      <td>sugar garlic onions vinegar green chilies grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moroccan</td>\n",
       "      <td>[raw pistachios, purple onion, couscous, dried...</td>\n",
       "      <td>[raw, pistachios, purple, onion, couscous, dri...</td>\n",
       "      <td>raw pistachios purple onion couscous dried apr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[tomatoes, pepper, salsa, sliced green onions,...</td>\n",
       "      <td>[tomatoes, pepper, salsa, sliced, green, onion...</td>\n",
       "      <td>tomatoes pepper salsa sliced green onions ched...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cuisine                                        ingredients  \\\n",
       "0   mexican  [shredded cheddar cheese, chicken meat, choppe...   \n",
       "1    indian  [fresh cilantro, purple onion, ground coriande...   \n",
       "2  filipino  [sugar, garlic, onions, vinegar, green chilies...   \n",
       "3  moroccan  [raw pistachios, purple onion, couscous, dried...   \n",
       "4   mexican  [tomatoes, pepper, salsa, sliced green onions,...   \n",
       "\n",
       "                                       ingre_splited  \\\n",
       "0  [shredded, cheddar, cheese, chicken, meat, cho...   \n",
       "1  [fresh, cilantro, purple, onion, ground, coria...   \n",
       "2  [sugar, garlic, onions, vinegar, green, chilie...   \n",
       "3  [raw, pistachios, purple, onion, couscous, dri...   \n",
       "4  [tomatoes, pepper, salsa, sliced, green, onion...   \n",
       "\n",
       "                                        ingre_string  \n",
       "0  shredded cheddar cheese chicken meat chopped o...  \n",
       "1  fresh cilantro purple onion ground coriander g...  \n",
       "2  sugar garlic onions vinegar green chilies grou...  \n",
       "3  raw pistachios purple onion couscous dried apr...  \n",
       "4  tomatoes pepper salsa sliced green onions ched...  "
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:25.002898Z",
     "start_time": "2019-04-01T19:55:24.999197Z"
    }
   },
   "outputs": [],
   "source": [
    "list_corpus = data_train['ingre_string'].tolist() # generate the corpus based on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:25.910594Z",
     "start_time": "2019-04-01T19:55:25.906554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shredded cheddar cheese chicken meat chopped onion tomatoes green onions black olives dried parsley flour tortillas diced tomatoes sour cream tomato sauce chili powder salsa dried oregano',\n",
       " 'fresh cilantro purple onion ground coriander ground turmeric ground ginger vegetable oil brown rice flour mustard seeds whole wheat flour rice vinegar cumin seed ground cumin water garlic cayenne pepper white sugar',\n",
       " 'sugar garlic onions vinegar green chilies ground black pepper coconut cream chicken fish sauce cooking oil coconut milk',\n",
       " 'raw pistachios purple onion couscous dried apricot lemon juice olive oil salt harissa chopped parsley',\n",
       " 'tomatoes pepper salsa sliced green onions cheddar cheese garlic powder baked tortilla chips taco sauce non-fat sour cream iceberg lettuce ground round kidney beans whole kernel corn, drain']"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:27.368181Z",
     "start_time": "2019-04-01T19:55:26.816356Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() # it comes from the built-in method in skilearn. TD-IDF\n",
    "vector = vectorizer.fit_transform(list_corpus)\n",
    "names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:28.290579Z",
     "start_time": "2019-04-01T19:55:28.287467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2867\n"
     ]
    }
   ],
   "source": [
    "print(len(names)) # count how many feature we constructed by the bag of words model(TD-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:29.224942Z",
     "start_time": "2019-04-01T19:55:29.222244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some words are meaningless like 'a' 'is' 'or' 'not', we can then delete them from features.\n",
    "custom_stop_words = []\n",
    "for word in ENGLISH_STOP_WORDS:\n",
    "    custom_stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:30.700751Z",
     "start_time": "2019-04-01T19:55:30.137405Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can generate features once again, but no stop words('is','or','and'...) in the features.\n",
    "vectorizer_use_sw = TfidfVectorizer(stop_words=custom_stop_words)\n",
    "vector_use_sw = vectorizer_use_sw.fit_transform(list_corpus)\n",
    "names_use_sw = vectorizer_use_sw.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:31.622197Z",
     "start_time": "2019-04-01T19:55:31.619112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831\n"
     ]
    }
   ],
   "source": [
    "print(len(names_use_sw)) # count how many feature we constructed if we delete the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:32.694248Z",
     "start_time": "2019-04-01T19:55:32.506603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ymyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ymyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Then some words are same like 'apple' and 'apples', we want to only keep 'apple'\n",
    "import nltk # we use the nltk package\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('dogs')) # it is a toy example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:36.037158Z",
     "start_time": "2019-04-01T19:55:33.589885Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_lemmatize(list_input):\n",
    "    list_get =[lemmatizer.lemmatize(word) for word in list_input]\n",
    "    return list_get\n",
    "\n",
    "# we use this method to update our splited words and then combine them into a sentence once again.\n",
    "data_train['ingre_splited'] = data_train['ingre_splited'].apply(token_lemmatize)\n",
    "data_train['ingre_string'] = data_train['ingre_splited'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:36.965074Z",
     "start_time": "2019-04-01T19:55:36.950926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingre_splited</th>\n",
       "      <th>ingre_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[shredded cheddar cheese, chicken meat, choppe...</td>\n",
       "      <td>[shredded, cheddar, cheese, chicken, meat, cho...</td>\n",
       "      <td>shredded cheddar cheese chicken meat chopped o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian</td>\n",
       "      <td>[fresh cilantro, purple onion, ground coriande...</td>\n",
       "      <td>[fresh, cilantro, purple, onion, ground, coria...</td>\n",
       "      <td>fresh cilantro purple onion ground coriander g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>[sugar, garlic, onions, vinegar, green chilies...</td>\n",
       "      <td>[sugar, garlic, onion, vinegar, green, chilies...</td>\n",
       "      <td>sugar garlic onion vinegar green chilies groun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moroccan</td>\n",
       "      <td>[raw pistachios, purple onion, couscous, dried...</td>\n",
       "      <td>[raw, pistachio, purple, onion, couscous, drie...</td>\n",
       "      <td>raw pistachio purple onion couscous dried apri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mexican</td>\n",
       "      <td>[tomatoes, pepper, salsa, sliced green onions,...</td>\n",
       "      <td>[tomato, pepper, salsa, sliced, green, onion, ...</td>\n",
       "      <td>tomato pepper salsa sliced green onion cheddar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cuisine                                        ingredients  \\\n",
       "0   mexican  [shredded cheddar cheese, chicken meat, choppe...   \n",
       "1    indian  [fresh cilantro, purple onion, ground coriande...   \n",
       "2  filipino  [sugar, garlic, onions, vinegar, green chilies...   \n",
       "3  moroccan  [raw pistachios, purple onion, couscous, dried...   \n",
       "4   mexican  [tomatoes, pepper, salsa, sliced green onions,...   \n",
       "\n",
       "                                       ingre_splited  \\\n",
       "0  [shredded, cheddar, cheese, chicken, meat, cho...   \n",
       "1  [fresh, cilantro, purple, onion, ground, coria...   \n",
       "2  [sugar, garlic, onion, vinegar, green, chilies...   \n",
       "3  [raw, pistachio, purple, onion, couscous, drie...   \n",
       "4  [tomato, pepper, salsa, sliced, green, onion, ...   \n",
       "\n",
       "                                        ingre_string  \n",
       "0  shredded cheddar cheese chicken meat chopped o...  \n",
       "1  fresh cilantro purple onion ground coriander g...  \n",
       "2  sugar garlic onion vinegar green chilies groun...  \n",
       "3  raw pistachio purple onion couscous dried apri...  \n",
       "4  tomato pepper salsa sliced green onion cheddar...  "
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:37.887907Z",
     "start_time": "2019-04-01T19:55:37.880636Z"
    }
   },
   "outputs": [],
   "source": [
    "# we update the corpus once again.\n",
    "list_corpus = data_train['ingre_string'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:38.822329Z",
     "start_time": "2019-04-01T19:55:38.818404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shredded cheddar cheese chicken meat chopped onion tomato green onion black olive dried parsley flour tortilla diced tomato sour cream tomato sauce chili powder salsa dried oregano',\n",
       " 'fresh cilantro purple onion ground coriander ground turmeric ground ginger vegetable oil brown rice flour mustard seed whole wheat flour rice vinegar cumin seed ground cumin water garlic cayenne pepper white sugar',\n",
       " 'sugar garlic onion vinegar green chilies ground black pepper coconut cream chicken fish sauce cooking oil coconut milk',\n",
       " 'raw pistachio purple onion couscous dried apricot lemon juice olive oil salt harissa chopped parsley',\n",
       " 'tomato pepper salsa sliced green onion cheddar cheese garlic powder baked tortilla chip taco sauce non-fat sour cream iceberg lettuce ground round kidney bean whole kernel corn, drain']"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:40.405372Z",
     "start_time": "2019-04-01T19:55:39.761313Z"
    }
   },
   "outputs": [],
   "source": [
    "# again, we generate the features using TD-IDF(a kind of the bag of words model)\n",
    "vectorizer_use_sw_lem = TfidfVectorizer(stop_words = custom_stop_words)\n",
    "vector_use_sw_lem = vectorizer_use_sw_lem.fit_transform(list_corpus)\n",
    "names_use_sw_lem = vectorizer_use_sw_lem.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:41.304705Z",
     "start_time": "2019-04-01T19:55:41.301791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2662\n"
     ]
    }
   ],
   "source": [
    "print(len(names_use_sw_lem)) # we count how many features we still have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:42.850019Z",
     "start_time": "2019-04-01T19:55:42.288184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7up' 'abalone' 'abbamele' ... 'ziti' 'zucchini' 'épices']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(names_use_sw_lem) \n",
    "print(feature_names)\n",
    "print(vector_use_sw_lem.toarray()) \n",
    "# the size of array is 31819*2662 ,31819 is the size of train data, 2662 is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:44.336692Z",
     "start_time": "2019-04-01T19:55:43.807430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salt' 'oil' 'pepper' 'garlic' 'onion' 'fresh' 'ground' 'olive' 'sugar'\n",
      " 'sauce' 'black' 'tomato' 'water' 'chicken' 'cheese' 'butter' 'flour'\n",
      " 'egg' 'green' 'red' 'clove' 'white' 'powder' 'juice' 'chopped' 'leaf'\n",
      " 'vegetable' 'cilantro' 'milk' 'rice']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer_use_sw_lem.idf_) \n",
    "# get the index of features which are ordered by their idf values\n",
    "\n",
    "print(feature_names[sorted_by_idf[:30]]) \n",
    "# idf values means how frequent it exists in recipes. now we present the most frequent 30 features.\n",
    "\n",
    "# As these features are so common that it may make a little contribution to the prediction.\n",
    "# We consider to delete them by adding these features into our customized stop words.\n",
    "\n",
    "for word in feature_names[sorted_by_idf[:30]]:\n",
    "    custom_stop_words.append(word)\n",
    "    \n",
    "# again, we generate the features using TD-IDF(a kind of the bag of words model)\n",
    "vectorizer_use_sw_lem = TfidfVectorizer(stop_words = custom_stop_words)\n",
    "vector_use_sw_lem = vectorizer_use_sw_lem.fit_transform(list_corpus)\n",
    "names_use_sw_lem = vectorizer_use_sw_lem.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:46.077422Z",
     "start_time": "2019-04-01T19:55:45.272326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7up' 'abalone' 'abbamele' ... 'ziti' 'zucchini' 'épices']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(31819, 2632)\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(names_use_sw_lem) \n",
    "print(feature_names)\n",
    "print(vector_use_sw_lem.toarray())\n",
    "print(np.shape(vector_use_sw_lem.toarray()))\n",
    "# the size of array is 31819*2632 ,31819 is the size of train data, 2632 is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T19:55:46.972814Z",
     "start_time": "2019-04-01T19:55:46.970662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then, I believe it would be much better if we continue to using SVD/PCA method to reduce the dimension of feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 416.903818,
   "position": {
    "height": "438.722px",
    "left": "910.074px",
    "right": "20px",
    "top": "151.966px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
